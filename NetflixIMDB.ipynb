{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NetflixIMDB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KyoldFusion/IMDB-Netflix-/blob/main/NetflixIMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf0vnl8G49kF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d23d3da3-4f90-4208-e1da-a6a894149fd7"
      },
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.1'\n",
        "spark_version = 'spark-3.1.1'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connected to cloud.r-pro\r0% [1 InRelease gpgv 88.7 kB] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Connecting to ppa.launchpa\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Connecting to ppa.launchpa\r                                                                               \rIgn:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Connecting to ppa.launchpa\r                                                                               \rHit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Connecting to ppa.launchpa\r                                                                               \rHit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teYG79a44_fE"
      },
      "source": [
        " #import packages\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructType,StructField,StringType, DateType,IntegerType\n",
        "\n",
        "# we are going to use this to time our queries.\n",
        "import time\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8JmO_ag5BAd"
      },
      "source": [
        "# Read in data from S3 Bucket\n",
        "from pyspark import SparkFiles\n",
        "ratings = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-netflix/ratings.csv\"\n",
        "spark.sparkContext.addFile(ratings)\n",
        "rf = spark.read.csv(SparkFiles.get(\"ratings.csv\"), sep=\",\", header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2OWO0cmAnCZ"
      },
      "source": [
        "rf.createOrReplaceTempView('ratings')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tonCTjPjATFm"
      },
      "source": [
        "basics = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-netflix/titles_basic.csv\"\n",
        "spark.sparkContext.addFile(basics)\n",
        "bf = spark.read.csv(SparkFiles.get(\"titles_basic.csv\"), sep=\",\", header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WprroUlfA1Du"
      },
      "source": [
        "bf.createOrReplaceTempView('basics')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEroly6eAbP0"
      },
      "source": [
        "names = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-netflix/filterNames.csv\"\n",
        "spark.sparkContext.addFile(names)\n",
        "nf = spark.read.csv(SparkFiles.get(\"filterNames.csv\"), sep=\",\", header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwjVGphOA4m4"
      },
      "source": [
        "nf.createOrReplaceTempView('names')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwdKB8obAhJH"
      },
      "source": [
        "principals = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-netflix/filteredPrincipals.csv\"\n",
        "spark.sparkContext.addFile(principals)\n",
        "pf = spark.read.csv(SparkFiles.get(\"filteredPrincipals.csv\"), sep=\",\", header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUiDt0iJA9b-"
      },
      "source": [
        "pf.createOrReplaceTempView('principals')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C23pEWRBCQ7",
        "outputId": "2b887fae-ef8c-468b-ad7a-ea9ff5337002"
      },
      "source": [
        "spark.sql(\"Select * from principals\").show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------+---------+--------+----+-----------------+\n",
            "|    tconst|ordering|   nconst|category| job|       characters|\n",
            "+----------+--------+---------+--------+----+-----------------+\n",
            "|tt12189034|       6|nm5943320|   actor|null|      \"[\"\"Lee\"\"]\"|\n",
            "|tt12189034|       6|nm5943320|   actor|null|      \"[\"\"Lee\"\"]\"|\n",
            "|tt12189034|       6|nm5943320|   actor|null|      \"[\"\"Lee\"\"]\"|\n",
            "|tt12189034|       6|nm5943320|   actor|null|      \"[\"\"Lee\"\"]\"|\n",
            "|tt12189034|       8|nm7672770|   actor|null|\"[\"\"Detective\"\"]\"|\n",
            "+----------+--------+---------+--------+----+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnSINn-2BFqQ",
        "outputId": "a7c60fb8-e5b6-4e0a-aa62-7216bb4c2eac"
      },
      "source": [
        "spark.sql(\"Select DISTINCT nconst, primaryname, birthyear, deathyear, CASE WHEN birthyear IS NOT NULL and deathyear IS NOT NULL then CAST(deathyear - birthyear AS decimal(3,0)) ELSE COALESCE((birthyear - deathyear), 2021-birthyear) END  Age, CASE WHEN deathyear IS NULL AND birthyear IS NOT NULL THEN 'alive' WHEN deathyear IS NOT NULL THEN 'deceased' ELSE 'unknown'  END LivingStatus from names where (primaryprofession LIKE '%actor%' OR primaryprofession LIKE '%actress%') AND birthyear IS NOT NULL\").show(20)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+--------------------+---------+---------+----+------------+\n",
            "|   nconst|         primaryname|birthyear|deathyear| Age|LivingStatus|\n",
            "+---------+--------------------+---------+---------+----+------------+\n",
            "|nm1441958|           Geddy Lee|     1953|     null|68.0|       alive|\n",
            "|nm1443104|   Wanda Wilkomirska|     1929|     2018|89.0|    deceased|\n",
            "|nm1450441|         Lexi Lamour|     1976|     null|45.0|       alive|\n",
            "|nm1451129| Carles Romero Vidal|     1977|     null|44.0|       alive|\n",
            "|nm1454258|       Cilly Dartell|     1957|     null|64.0|       alive|\n",
            "|nm6268945|   Monika Potokárová|     1992|     2019|27.0|    deceased|\n",
            "|nm6276162|    Alasdair Gillies|     1938|     null|83.0|       alive|\n",
            "|nm6278631|               Spice|     1982|     null|39.0|       alive|\n",
            "|nm0000139|        Cameron Diaz|     1972|     null|49.0|       alive|\n",
            "|nm0000457|           John Hurt|     1940|     2017|77.0|    deceased|\n",
            "|nm0002168|Willeke van Ammel...|     1944|     null|77.0|       alive|\n",
            "|nm1457058|        Melanie Teix|     1983|     null|38.0|       alive|\n",
            "|nm0000128|       Russell Crowe|     1964|     null|57.0|       alive|\n",
            "|nm0001774|         Ben Stiller|     1965|     null|56.0|       alive|\n",
            "|nm0467027| Margarita Kosheleva|     1939|     2015|76.0|    deceased|\n",
            "|nm0000961|     Timothy Bottoms|     1951|     null|70.0|       alive|\n",
            "|nm0001808|       Tracey Ullman|     1959|     null|62.0|       alive|\n",
            "|nm0003012|Michael Creighton...|     1963|     null|58.0|       alive|\n",
            "|nm0003595|        Corey Klemow|     1970|     null|51.0|       alive|\n",
            "|nm0004963|         Peri Gilpin|     1961|     null|60.0|       alive|\n",
            "+---------+--------------------+---------+---------+----+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "fxGfAgscKaJy",
        "outputId": "a1b67cbb-bda3-4b6a-976f-567467219891"
      },
      "source": [
        "spark.sql(\"SELECT DISTINCT nconst, primaryname, COALESCE((birthyear - deathyear), 2021-birthyear) age from names WHERE birthyear IS NOT NULL AND nconst = 'nm0459889'\").show(10)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-c88f9dc235ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT DISTINCT nconst, primaryname, COALESCE((birthyear - deathyear), GETDATE()-birthyear) age from names WHERE birthyear IS NOT NULL AND nconst = 'nm0459889'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Undefined function: 'GETDATE'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 71"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxkvNaf-BIYu",
        "outputId": "8c3a4abf-187d-4ad4-fcc2-4256ab5656ee"
      },
      "source": [
        "spark.sql(\"Select * from ratings\").show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-------------+--------+\n",
            "|   tconst|averagerating|numvotes|\n",
            "+---------+-------------+--------+\n",
            "|tt0214461|          7.4|      74|\n",
            "|tt0214659|          6.0|      21|\n",
            "|tt0214878|          6.1|    1566|\n",
            "|tt0215244|          3.0|       5|\n",
            "|tt0215402|          8.1|       9|\n",
            "+---------+-------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq2V7frvBJld",
        "outputId": "d530ef5e-5599-4853-dd59-4e41828902a4"
      },
      "source": [
        "spark.sql(\"Select * from basics\").show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+-----------------+\n",
            "|_c0|   tconst|titletype|        primarytitle|       originaltitle|isadult|startyear|endyear|runtimeminutes|           genres|\n",
            "+---+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+-----------------+\n",
            "|  0|tt0439997|    movie|           500 Almas|           500 Almas|      0|     2004|   null|         105.0|      Documentary|\n",
            "|  1|tt0439999|tvSpecial|           80s Mania|           80s Mania|      0|     2001|   null|          50.0|            Music|\n",
            "|  3|tt0440003|    movie|        A-1 Headline|         A1 tou tiao|      0|     2004|   null|          95.0| Mystery,Thriller|\n",
            "|  4|tt0440004|  tvMovie| AD/BC: A Rock Opera| AD/BC: A Rock Opera|      0|     2004|   null|          30.0|   Comedy,Musical|\n",
            "|  5|tt0440008|  tvMovie|Abbamania: We Say...|Abbamania: We Say...|      0|     2004|   null|          50.0|Documentary,Music|\n",
            "+---+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51Vz7Yu8BM31",
        "outputId": "7e8180b5-e3bf-4df4-b26f-62b19aee75e3"
      },
      "source": [
        "spark.sql(\"Select r.tconst from ratings r join basics b on r.tconst = b.tconst join principals p on p.tconst = b.tconst join names n on p.nconst = n.nconst\").show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|   tconst|\n",
            "+---------+\n",
            "|tt1345836|\n",
            "|tt1345836|\n",
            "|tt1345836|\n",
            "|tt1345836|\n",
            "|tt1345836|\n",
            "+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNuF1FirBza2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}